{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12442208,"sourceType":"datasetVersion","datasetId":7848647},{"sourceId":12442366,"sourceType":"datasetVersion","datasetId":7848744},{"sourceId":12445018,"sourceType":"datasetVersion","datasetId":7850335}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install torchvision\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:29:51.566748Z","iopub.execute_input":"2025-07-12T01:29:51.567048Z","iopub.status.idle":"2025-07-12T01:29:58.837884Z","shell.execute_reply.started":"2025-07-12T01:29:51.567026Z","shell.execute_reply":"2025-07-12T01:29:58.836553Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!pip install opencv-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:29:58.840046Z","iopub.execute_input":"2025-07-12T01:29:58.840380Z","iopub.status.idle":"2025-07-12T01:30:02.950060Z","shell.execute_reply.started":"2025-07-12T01:29:58.840352Z","shell.execute_reply":"2025-07-12T01:30:02.948757Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!pip install pdf2image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:02.951245Z","iopub.execute_input":"2025-07-12T01:30:02.951823Z","iopub.status.idle":"2025-07-12T01:30:06.610268Z","shell.execute_reply.started":"2025-07-12T01:30:02.951788Z","shell.execute_reply":"2025-07-12T01:30:06.608906Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!pip install pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:06.613112Z","iopub.execute_input":"2025-07-12T01:30:06.613456Z","iopub.status.idle":"2025-07-12T01:30:10.330916Z","shell.execute_reply.started":"2025-07-12T01:30:06.613423Z","shell.execute_reply":"2025-07-12T01:30:10.329691Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import fitz  # PyMuPDF\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.332190Z","iopub.execute_input":"2025-07-12T01:30:10.332560Z","iopub.status.idle":"2025-07-12T01:30:10.339209Z","shell.execute_reply.started":"2025-07-12T01:30:10.332523Z","shell.execute_reply":"2025-07-12T01:30:10.338142Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Path to your uploaded PDF\npdf_path = \"/kaggle/input/exam-paper/iot1 004.pdf\"  # CHANGE THIS\n\n# Open the PDF\ndoc = fitz.open(pdf_path)\n\n# Get total page count\nprint(f\"📄 Total Pages: {len(doc)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.340481Z","iopub.execute_input":"2025-07-12T01:30:10.340803Z","iopub.status.idle":"2025-07-12T01:30:10.365148Z","shell.execute_reply.started":"2025-07-12T01:30:10.340780Z","shell.execute_reply":"2025-07-12T01:30:10.363989Z"}},"outputs":[{"name":"stdout","text":"📄 Total Pages: 9\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Load TR-OCR model\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.366300Z","iopub.execute_input":"2025-07-12T01:30:10.366777Z","iopub.status.idle":"2025-07-12T01:30:12.303755Z","shell.execute_reply.started":"2025-07-12T01:30:10.366673Z","shell.execute_reply":"2025-07-12T01:30:12.302730Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"def extract_lines_from_image(pil_image):\n    img = np.array(pil_image)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\n    # Threshold to binary\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Dilation to connect text lines\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (img.shape[1] // 30, 5))\n    dilated = cv2.dilate(binary, kernel, iterations=1)\n\n    # Find contours (text lines)\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    line_images = []\n\n    for cnt in sorted(contours, key=lambda c: cv2.boundingRect(c)[1]):  # top to bottom\n        x, y, w, h = cv2.boundingRect(cnt)\n        line = img[y:y + h, x:x + w]\n        pil_line = Image.fromarray(line).convert(\"RGB\")\n        line_images.append(pil_line)\n\n    return line_images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:12.304916Z","iopub.execute_input":"2025-07-12T01:30:12.305226Z","iopub.status.idle":"2025-07-12T01:30:12.313750Z","shell.execute_reply.started":"2025-07-12T01:30:12.305204Z","shell.execute_reply":"2025-07-12T01:30:12.312171Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"for i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)\n    page_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n\n    print(f\"\\n📄 Page {i+1}\")\n\n    lines = extract_lines_from_image(page_img)\n\n    for idx, line_img in enumerate(lines):\n        # Resize line image to improve OCR\n        line_img = line_img.resize((line_img.width * 2, line_img.height * 2))\n\n        # OCR\n        inputs = processor(images=line_img, return_tensors=\"pt\").pixel_values.to(device)\n        generated_ids = model.generate(inputs)\n        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        print(f\"🖋️ Line {idx+1}: {text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:12.315138Z","iopub.execute_input":"2025-07-12T01:30:12.315471Z","execution_failed":"2025-07-12T02:28:28.677Z"}},"outputs":[{"name":"stdout","text":"\n📄 Page 1\n🖋️ Line 1: 0\n🖋️ Line 2: 0 0\n🖋️ Line 3: 0 0\n🖋️ Line 4: backbone of\n🖋️ Line 5: 4ft.\n🖋️ Line 6: cocks .\n🖋️ Line 7: pyroprietentials\n🖋️ Line 8: 705\n🖋️ Line 9: I commemorate initiators\n🖋️ Line 10: 0 0\n🖋️ Line 11: songs of the\n🖋️ Line 12: Print export\n🖋️ Line 13: networkinterface\n🖋️ Line 14: 0/\n🖋️ Line 15: dynamic\n🖋️ Line 16: getting\n🖋️ Line 17: jig\n🖋️ Line 18: of its\n🖋️ Line 19: 0 0\n🖋️ Line 20: of some\n🖋️ Line 21: created\n🖋️ Line 22: 2 2\n🖋️ Line 23: 1915.\n🖋️ Line 24: space\n🖋️ Line 25: 0 0\n🖋️ Line 26: vinters .\n🖋️ Line 27: puffhere\n🖋️ Line 28: 0 0\n🖋️ Line 29: pres-connected\n🖋️ Line 30: minimumization .\n🖋️ Line 31: 0/ 0of.\n🖋️ Line 32: 0 0\n🖋️ Line 33: clata\n🖋️ Line 34: the\n🖋️ Line 35: 0 0\n🖋️ Line 36: civersion .\n🖋️ Line 37: connected\n🖋️ Line 38: auto\n🖋️ Line 39: devices .\n🖋️ Line 40: the\n🖋️ Line 41: devices .\n🖋️ Line 42: loydy\n🖋️ Line 43: cannied .\n🖋️ Line 44: is\n🖋️ Line 45: communication\n🖋️ Line 46: this\n🖋️ Line 47: \" Internet .\n🖋️ Line 48: in an environment .\n🖋️ Line 49: 0 0\n🖋️ Line 50: 0 0\n🖋️ Line 51: each devices .\n🖋️ Line 52: white .\n🖋️ Line 53: I protocols .\n🖋️ Line 54: 0 0\n🖋️ Line 55: palentities ;\n🖋️ Line 56: unique\n🖋️ Line 57: 0.\n🖋️ Line 58: has\n🖋️ Line 59: 0 0\n🖋️ Line 60: 0 0\n🖋️ Line 61: whether\n🖋️ Line 62: will ensure\n🖋️ Line 63: protocols .\n🖋️ Line 64: 0 0\n🖋️ Line 65: 1st Communication\n🖋️ Line 66: connected 1Interlinked in an\n🖋️ Line 67: darve .\n🖋️ Line 68: devices .\n🖋️ Line 69: part of malating\n🖋️ Line 70: 1 the\n🖋️ Line 71: 0 0\n🖋️ Line 72: cordial\n🖋️ Line 73: 0 0\n🖋️ Line 74: a\n🖋️ Line 75: this\n🖋️ Line 76: LS\n🖋️ Line 77: 0 0\n🖋️ Line 78: \" and virtual environment .\n🖋️ Line 79: I internet .\n🖋️ Line 80: 0 0\n🖋️ Line 81: 0 0\n🖋️ Line 82: 0 0\n🖋️ Line 83: 0 0\n🖋️ Line 84: 0 0\n🖋️ Line 85: 0 0\n🖋️ Line 86: 0 0\n🖋️ Line 87: 0 1\n🖋️ Line 88: hardware .\n🖋️ Line 89: 0 0\n🖋️ Line 90: 0 0\n🖋️ Line 91: when there is no\n🖋️ Line 92: jephysical\n🖋️ Line 93: 0 0\n🖋️ Line 94: 0 1 .\n🖋️ Line 95: songs for\n🖋️ Line 96: 0 0\n🖋️ Line 97: Lightening\n🖋️ Line 98: sensons .\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"for i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)\n    page_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n\n    print(f\"\\n📄 Page {i+1}\")\n\n    lines = extract_lines_from_image(page_img)\n\n    for idx, line_img in enumerate(lines):\n        # Resize line image to improve OCR\n        line_img = line_img.resize((line_img.width * 2, line_img.height * 2))\n\n        # OCR\n        inputs = processor(images=line_img, return_tensors=\"pt\").pixel_values.to(device)\n        generated_ids = model.generate(inputs)\n        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        print(f\"🖋️ Line {idx+1}: {text}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Loop through pages\nfor i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)  # Render page to image at 300 DPI\n\n    # Convert to PIL\n    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n\n    # Convert PIL to OpenCV format\n    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n    # Preprocessing\n    gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\n    denoised = cv2.fastNlMeansDenoising(gray, h=30)\n    sharpened = cv2.filter2D(denoised, -1, np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]))\n    thresh = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                   cv2.THRESH_BINARY, 11, 2)\n    resized = cv2.resize(thresh, (thresh.shape[1]*2, thresh.shape[0]*2))\n    preprocessed_image = Image.fromarray(resized).convert(\"RGB\") \n\n    # Show preview (optional)\n    plt.imshow(preprocessed_image, cmap='gray')\n    plt.axis(\"off\")\n    plt.title(f\"Preprocessed Page {i+1}\")\n    plt.show()\n\n    # Run TR-OCR\n    pixel_values = processor(images=preprocessed_image, return_tensors=\"pt\").pixel_values.to(device)\n    generated_ids = model.generate(pixel_values)\n    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(f\"\\n📄 Extracted Text - Page {i+1}:\\n{text}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load sample image from a URL\nimport requests\n\nimage = Image.open(\"/kaggle/input/handwriting/dsa-new-_00005.jpg\").convert(\"RGB\")\n\n# Show image\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_cv = np.array(image)\nimage_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n\ngray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\n\n# Adaptive Thresholding\nthresh = cv2.adaptiveThreshold(\n    gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n)\n\n# Resize (scale up)\nscale = 2\nheight, width = thresh.shape\nresized = cv2.resize(thresh, (width * scale, height * scale), interpolation=cv2.INTER_LINEAR)\n\n# Convert back to PIL (required by TrOCR)\npreprocessed_image = Image.fromarray(resized)\n\n# Show preprocessed image\nplt.imshow(preprocessed_image, cmap='gray')\nplt.axis(\"off\")\nplt.title(\"Preprocessed Image\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load processor and model\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\n\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess and run model\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\ngenerated_ids = model.generate(pixel_values)\n\n# Decode and print\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(\"📝 Extracted Text:\\n\", generated_text)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T02:28:28.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}