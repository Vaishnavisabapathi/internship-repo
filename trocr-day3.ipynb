{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12445018,"sourceType":"datasetVersion","datasetId":7850335}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:27:02.732791Z","iopub.execute_input":"2025-07-13T19:27:02.732985Z","iopub.status.idle":"2025-07-13T19:28:32.620971Z","shell.execute_reply.started":"2025-07-13T19:27:02.732968Z","shell.execute_reply":"2025-07-13T19:28:32.620082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:28:39.831233Z","iopub.execute_input":"2025-07-13T19:28:39.831894Z","iopub.status.idle":"2025-07-13T19:28:42.902968Z","shell.execute_reply.started":"2025-07-13T19:28:39.831845Z","shell.execute_reply":"2025-07-13T19:28:42.902054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdf2image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:28:55.035180Z","iopub.execute_input":"2025-07-13T19:28:55.035464Z","iopub.status.idle":"2025-07-13T19:28:58.200595Z","shell.execute_reply.started":"2025-07-13T19:28:55.035437Z","shell.execute_reply":"2025-07-13T19:28:58.199584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:29:00.862860Z","iopub.execute_input":"2025-07-13T19:29:00.863633Z","iopub.status.idle":"2025-07-13T19:29:06.540346Z","shell.execute_reply.started":"2025-07-13T19:29:00.863580Z","shell.execute_reply":"2025-07-13T19:29:06.539424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:29:26.311656Z","iopub.execute_input":"2025-07-13T19:29:26.312258Z","iopub.status.idle":"2025-07-13T19:30:00.272780Z","shell.execute_reply.started":"2025-07-13T19:29:26.312211Z","shell.execute_reply":"2025-07-13T19:30:00.272059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:30:06.035483Z","iopub.execute_input":"2025-07-13T19:30:06.036077Z","iopub.status.idle":"2025-07-13T19:30:38.567615Z","shell.execute_reply.started":"2025-07-13T19:30:06.036051Z","shell.execute_reply":"2025-07-13T19:30:38.566743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_lines_from_image(pil_image):\n    img = np.array(pil_image)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    kernel_width = max(20, img.shape[1] // 25)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width, 5))\n    dilated = cv2.dilate(binary, kernel, iterations=1)\n\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    line_images = []\n\n    for cnt in sorted(contours, key=lambda c: cv2.boundingRect(c)[1]):\n        x, y, w, h = cv2.boundingRect(cnt)\n        if h > 20 and w > 50:\n            line = img[y:y + h, x:x + w]\n            line_img = Image.fromarray(line).convert(\"RGB\")\n            line_images.append(line_img)\n\n    return line_images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:31:57.591945Z","iopub.execute_input":"2025-07-13T19:31:57.592477Z","iopub.status.idle":"2025-07-13T19:31:57.598420Z","shell.execute_reply.started":"2025-07-13T19:31:57.592453Z","shell.execute_reply":"2025-07-13T19:31:57.597705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_line_image(line_img):\n    img = np.array(line_img)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\n    # CLAHE for local contrast enhancement\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    contrast = clahe.apply(gray)\n\n    # Denoise\n    denoised = cv2.fastNlMeansDenoising(contrast, h=15)\n\n    # Sharpen\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5, -1],\n                       [0, -1, 0]])\n    sharpened = cv2.filter2D(denoised, -1, kernel)\n\n    # Adaptive threshold\n    binary = cv2.adaptiveThreshold(\n        sharpened, 255,\n        cv2.ADAPTIVE_THRESH_MEAN_C,\n        cv2.THRESH_BINARY_INV,\n        15, 8\n    )\n\n    # Resize for OCR (4Ã—)\n    resized = cv2.resize(binary, (binary.shape[1]*4, binary.shape[0]*4))\n    return Image.fromarray(resized).convert(\"RGB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:32:06.581573Z","iopub.execute_input":"2025-07-13T19:32:06.581875Z","iopub.status.idle":"2025-07-13T19:32:06.587774Z","shell.execute_reply.started":"2025-07-13T19:32:06.581852Z","shell.execute_reply":"2025-07-13T19:32:06.586964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_ocr_on_page(pil_image, page_number):\n    lines = extract_lines_from_image(pil_image)\n    print(f\"\\nðŸ“„ Page {page_number} â€” Total Detected Lines: {len(lines)}\")\n\n    for i, line in enumerate(lines):\n        processed = preprocess_line_image(line)\n\n        pixel_values = processor(images=processed, return_tensors=\"pt\").pixel_values.to(device)\n        generated_ids = model.generate(pixel_values, max_length=512)\n        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        # Filter gibberish lines\n        if len(text.strip()) > 3 and any(c.isalpha() for c in text):\n            print(f\"ðŸ–‹ï¸ Line {i+1}: {text.strip()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:32:30.220611Z","iopub.execute_input":"2025-07-13T19:32:30.221495Z","iopub.status.idle":"2025-07-13T19:32:30.226489Z","shell.execute_reply.started":"2025-07-13T19:32:30.221468Z","shell.execute_reply":"2025-07-13T19:32:30.225889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/exam-paper/iot1 004.pdf\"  # change this!\ndoc = fitz.open(pdf_path)\n\nfor i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)\n    pil_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n\n    run_ocr_on_page(pil_img, i + 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:33:29.917612Z","iopub.execute_input":"2025-07-13T19:33:29.918189Z","iopub.status.idle":"2025-07-13T19:35:51.197235Z","shell.execute_reply.started":"2025-07-13T19:33:29.918167Z","shell.execute_reply":"2025-07-13T19:35:51.196636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pymupdf transformers opencv-python pyspellchecker --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:39:18.378391Z","iopub.execute_input":"2025-07-13T19:39:18.379104Z","iopub.status.idle":"2025-07-13T19:39:22.169503Z","shell.execute_reply.started":"2025-07-13T19:39:18.379081Z","shell.execute_reply":"2025-07-13T19:39:22.168408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom spellchecker import SpellChecker\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:39:35.796669Z","iopub.execute_input":"2025-07-13T19:39:35.797507Z","iopub.status.idle":"2025-07-13T19:39:35.807605Z","shell.execute_reply.started":"2025-07-13T19:39:35.797475Z","shell.execute_reply":"2025-07-13T19:39:35.807057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nspell = SpellChecker()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:39:49.213440Z","iopub.execute_input":"2025-07-13T19:39:49.214089Z","iopub.status.idle":"2025-07-13T19:39:52.392310Z","shell.execute_reply.started":"2025-07-13T19:39:49.214053Z","shell.execute_reply":"2025-07-13T19:39:52.391421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_line_image(line_img):\n    img = np.array(line_img)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\n    # Contrast enhancement\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    contrast = clahe.apply(gray)\n\n    # Denoise\n    denoised = cv2.fastNlMeansDenoising(contrast, h=15)\n\n    # Sharpen\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened = cv2.filter2D(denoised, -1, kernel)\n\n    # Adaptive thresholding\n    binary = cv2.adaptiveThreshold(sharpened, 255,\n                                   cv2.ADAPTIVE_THRESH_MEAN_C,\n                                   cv2.THRESH_BINARY_INV, 15, 8)\n\n    # Resize (4x)\n    resized = cv2.resize(binary, (binary.shape[1]*4, binary.shape[0]*4))\n    return Image.fromarray(resized).convert(\"RGB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:40:03.342475Z","iopub.execute_input":"2025-07-13T19:40:03.343069Z","iopub.status.idle":"2025-07-13T19:40:03.348343Z","shell.execute_reply.started":"2025-07-13T19:40:03.343046Z","shell.execute_reply":"2025-07-13T19:40:03.347564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_lines_from_image(pil_image):\n    img = np.array(pil_image)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (img.shape[1]//25, 5))\n    dilated = cv2.dilate(binary, kernel, iterations=1)\n\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    lines = []\n\n    for cnt in sorted(contours, key=lambda c: cv2.boundingRect(c)[1]):\n        x, y, w, h = cv2.boundingRect(cnt)\n        if h > 20 and w > 50:\n            cropped = img[y:y+h, x:x+w]\n            lines.append(Image.fromarray(cropped).convert(\"RGB\"))\n    return lines\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:40:15.618439Z","iopub.execute_input":"2025-07-13T19:40:15.619318Z","iopub.status.idle":"2025-07-13T19:40:15.625667Z","shell.execute_reply.started":"2025-07-13T19:40:15.619290Z","shell.execute_reply":"2025-07-13T19:40:15.624827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def correct_spelling(text):\n    words = text.split()\n    corrected = [\n        spell.correction(word) if spell.correction(word) is not None else word\n        for word in words\n    ]\n    return ' '.join(corrected)\n\ndef run_ocr_on_page(pil_image, page_num):\n    lines = extract_lines_from_image(pil_image)\n    print(f\"\\nðŸ“„ Page {page_num} - Total Lines: {len(lines)}\")\n\n    for i, line in enumerate(lines):\n        processed = preprocess_line_image(line)\n        pixel_values = processor(images=processed, return_tensors=\"pt\").pixel_values.to(device)\n        generated_ids = model.generate(pixel_values, max_length=512)\n        text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n        if len(text) > 3 and any(c.isalpha() for c in text):\n            corrected = correct_spelling(text)\n            print(f\"ðŸ–‹ï¸ Line {i+1}: {corrected}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:42:04.969366Z","iopub.execute_input":"2025-07-13T19:42:04.969678Z","iopub.status.idle":"2025-07-13T19:42:04.976388Z","shell.execute_reply.started":"2025-07-13T19:42:04.969656Z","shell.execute_reply":"2025-07-13T19:42:04.975597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/exam-paper/iot1 004.pdf\"  # Change this\ndoc = fitz.open(pdf_path)\n\nfor i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)\n    pil_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n    run_ocr_on_page(pil_img, i + 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:42:08.997465Z","iopub.execute_input":"2025-07-13T19:42:08.997749Z","iopub.status.idle":"2025-07-13T19:45:16.038979Z","shell.execute_reply.started":"2025-07-13T19:42:08.997726Z","shell.execute_reply":"2025-07-13T19:45:16.038398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF\nfrom PIL import Image\nimport os\n\npdf_path = \"/kaggle/input/exam-paper/iot1 004.pdf\"\noutput_dir = \"page_images\"\nos.makedirs(output_dir, exist_ok=True)\n\ndoc = fitz.open(pdf_path)\n\nfor i in range(len(doc)):\n    page = doc[i]\n    pix = page.get_pixmap(dpi=300)\n    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n    img.save(f\"{output_dir}/page_{i+1:03}.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:56:47.182007Z","iopub.execute_input":"2025-07-13T19:56:47.182321Z","iopub.status.idle":"2025-07-13T19:56:52.842426Z","shell.execute_reply.started":"2025-07-13T19:56:47.182301Z","shell.execute_reply":"2025-07-13T19:56:52.841836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef segment_lines_from_page(image_path, save_dir, page_num):\n    os.makedirs(save_dir, exist_ok=True)\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Threshold for binary image\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Dilate horizontally to group lines\n   kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (int(img.shape[1] * 0.8), 5))\n\n    dilated = cv2.dilate(binary, kernel, iterations=1)\n\n    # Find line contours\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    line_id = 1\n\n    for cnt in sorted(contours, key=lambda c: cv2.boundingRect(c)[1]):\n        x, y, w, h = cv2.boundingRect(cnt)\n        if h > 20 and w > 50:  # ignore small noise\n            cropped = img[y:y+h, x:x+w]\n            out_path = os.path.join(save_dir, f\"page{page_num:03}_line{line_id:03}.png\")\n            cv2.imwrite(out_path, cropped)\n            line_id += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:06:06.215995Z","iopub.execute_input":"2025-07-13T20:06:06.216259Z","iopub.status.idle":"2025-07-13T20:06:06.220961Z","shell.execute_reply.started":"2025-07-13T20:06:06.216242Z","shell.execute_reply":"2025-07-13T20:06:06.220056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"line_dir = \"line_images\"\nos.makedirs(line_dir, exist_ok=True)\n\npage_images = sorted(os.listdir(\"page_images\"))\n\nfor i, filename in enumerate(page_images):\n    path = os.path.join(\"page_images\", filename)\n    segment_lines_from_page(path, line_dir, i + 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:57:21.703891Z","iopub.execute_input":"2025-07-13T19:57:21.704504Z","iopub.status.idle":"2025-07-13T19:57:23.524232Z","shell.execute_reply.started":"2025-07-13T19:57:21.704478Z","shell.execute_reply":"2025-07-13T19:57:23.523658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"Total lines:\", len(os.listdir(\"line_images\")))\nprint(os.listdir(\"line_images\")[:10])  # print a few\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:58:40.134945Z","iopub.execute_input":"2025-07-13T19:58:40.135262Z","iopub.status.idle":"2025-07-13T19:58:40.140949Z","shell.execute_reply.started":"2025-07-13T19:58:40.135240Z","shell.execute_reply":"2025-07-13T19:58:40.140206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport os\n\nfolder = \"line_images\"\nimages = sorted(os.listdir(folder))[:10]  # show first 10\n\nplt.figure(figsize=(10, 20))\nfor idx, img_name in enumerate(images):\n    img = cv2.imread(os.path.join(folder, img_name))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(10, 1, idx + 1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(img_name)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:01:36.146251Z","iopub.execute_input":"2025-07-13T20:01:36.146825Z","iopub.status.idle":"2025-07-13T20:01:37.099455Z","shell.execute_reply.started":"2025-07-13T20:01:36.146804Z","shell.execute_reply":"2025-07-13T20:01:37.098714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(device)\n\nline_dir = \"line_images\"\nimage_files = sorted(os.listdir(line_dir))\n\nfor i, file in enumerate(image_files):\n    image = Image.open(os.path.join(line_dir, file)).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n    generated_ids = model.generate(pixel_values)\n    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    print(f\"ðŸ“„ Line {i+1:03}: {text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:06:25.853809Z","iopub.execute_input":"2025-07-13T20:06:25.854076Z","iopub.status.idle":"2025-07-13T20:07:36.324224Z","shell.execute_reply.started":"2025-07-13T20:06:25.854058Z","shell.execute_reply":"2025-07-13T20:07:36.323428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_both_directions_ocr(image):\n    normal = image\n    flipped = image.transpose(Image.FLIP_LEFT_RIGHT)\n\n    def get_text(pil_img):\n        pixel_values = processor(images=pil_img, return_tensors=\"pt\").pixel_values.to(device)\n        ids = model.generate(pixel_values)\n        return processor.batch_decode(ids, skip_special_tokens=True)[0]\n\n    text_normal = get_text(normal)\n    text_flipped = get_text(flipped)\n\n    # Heuristic: choose the longer or more alphabetic one\n    if sum(c.isalpha() for c in text_flipped) > sum(c.isalpha() for c in text_normal):\n        return text_flipped\n    return text_normal\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:11:40.347745Z","iopub.execute_input":"2025-07-13T20:11:40.348087Z","iopub.status.idle":"2025-07-13T20:11:40.353812Z","shell.execute_reply.started":"2025-07-13T20:11:40.348063Z","shell.execute_reply":"2025-07-13T20:11:40.353067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y poppler-utils\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:19:26.816141Z","iopub.execute_input":"2025-07-13T20:19:26.816418Z","iopub.status.idle":"2025-07-13T20:19:37.382368Z","shell.execute_reply.started":"2025-07-13T20:19:26.816398Z","shell.execute_reply":"2025-07-13T20:19:37.381651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pdf2image import convert_from_path\nimport os\n\npdf_path = \"/kaggle/input/exam-paper/iot1 004.pdf\"  # <- adjust to your file's actual path\noutput_folder = \"page_images\"\nos.makedirs(output_folder, exist_ok=True)\n\n# Convert the first page to image\npages = convert_from_path(pdf_path, dpi=300)\npages[0].save(os.path.join(output_folder, \"page001.png\"))\n\nprint(\"âœ… PDF page converted to image successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:19:58.315785Z","iopub.execute_input":"2025-07-13T20:19:58.316610Z","iopub.status.idle":"2025-07-13T20:20:00.995557Z","shell.execute_reply.started":"2025-07-13T20:19:58.316568Z","shell.execute_reply":"2025-07-13T20:20:00.994784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:47:53.543828Z","iopub.execute_input":"2025-07-13T20:47:53.544410Z","iopub.status.idle":"2025-07-13T20:47:53.548381Z","shell.execute_reply.started":"2025-07-13T20:47:53.544384Z","shell.execute_reply":"2025-07-13T20:47:53.547810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load TR-OCR model and processor\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(device)\n\n# STEP 1 â€” Line segmentation from a single page image\ndef segment_lines_from_page(image_path, output_dir=\"line_images_page1\", page_num=1):\n    os.makedirs(output_dir, exist_ok=True)\n\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Binary inverse + dilation to detect horizontal lines\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (int(img.shape[1] * 0.3), 5))\n\n    dilated = cv2.dilate(binary, kernel, iterations=1)\n# Step 1: Find contours after dilation\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# âœ… Step 2: Define and filter oversized contours\n    filtered_contours = []\n    for c in contours:\n        x, y, w, h = cv2.boundingRect(c)\n        if h < 150:  # You can tune this value\n            filtered_contours.append(c)\n\n    # Step 3: Sort top-to-bottom\n    lines = sorted(filtered_contours, key=lambda c: cv2.boundingRect(c)[1])\n\n    # Step 4: Save cropped line images\n    for i, c in enumerate(lines):\n      x, y, w, h = cv2.boundingRect(c)\n      line_img = img[y:y + h, x:x + w]\n      cv2.imwrite(os.path.join(output_dir, f\"line_{i+1:03}.png\"), line_img)\n   \n\n    print(f\"âœ… Extracted {len(lines)} line images from page {page_num}\")\n\n# STEP 2 â€” Preprocess each line image for better OCR\ndef preprocess_line_for_ocr(pil_image):\n    image = np.array(pil_image)\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # CLAHE contrast enhancement\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    enhanced = clahe.apply(gray)\n\n    # Resize\n    resized = cv2.resize(enhanced, (1024, 256))\n\n    # Convert single-channel grayscale back to 3-channel RGB\n    rgb_resized = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)\n    flipped_rgb = cv2.flip(rgb_resized, 1)\n\n    return Image.fromarray(rgb_resized), Image.fromarray(flipped_rgb)\n   \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:53:41.210851Z","iopub.execute_input":"2025-07-13T20:53:41.211178Z","iopub.status.idle":"2025-07-13T20:53:43.292538Z","shell.execute_reply.started":"2025-07-13T20:53:41.211157Z","shell.execute_reply":"2025-07-13T20:53:43.291966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n# STEP 3 â€” Smart OCR: Try both directions, pick better\ndef run_both_directions_ocr(image_normal, image_flipped):\n    def get_text(pil_img):\n        pixel_values = processor(images=pil_img, return_tensors=\"pt\").pixel_values.to(device)\n        ids = model.generate(pixel_values)\n        return processor.batch_decode(ids, skip_special_tokens=True)[0]\n\n    text_normal = get_text(image_normal)\n    text_flipped = get_text(image_flipped)\n\n    # Choose better (heuristic)\n    if sum(c.isalpha() for c in text_flipped) > sum(c.isalpha() for c in text_normal):\n        return text_flipped\n    return text_normal\n\n# STEP 4 â€” Run all steps on one page\npage_image_path = \"page_images/page001.png\"  # Replace with your actual path\nline_dir = \"line_images_page1\"\n\nsegment_lines_from_page(page_image_path, line_dir, page_num=1)\n\nprint(\"\\nðŸ§  Extracting text from lines...\\n\")\nimage_files = sorted(os.listdir(line_dir))\nseen_lines = set()  # ðŸ‘ˆ Add this at the top\n\n\nfor i, filename in enumerate(image_files):\n    image_path = os.path.join(line_dir, filename)\n    image = Image.open(image_path).convert(\"RGB\")\n\n    pre_normal, pre_flipped = preprocess_line_for_ocr(image)\n    text = run_both_directions_ocr(pre_normal, pre_flipped)\n    if len(text.strip()) < 3 or sum(c.isalpha() for c in text) < 3:\n        continue  # skip garbage lines\n\n    if text.strip() in seen_lines:\n        continue  # skip duplicate lines\n\n    seen_lines.add(text.strip())\n\n    if len(text.strip()) > 0:\n        print(f\"ðŸ“„ Line {i+1:03}: {text}\")\n    else:\n        print(f\"ðŸ“„ Line {i+1:03}: [EMPTY/NO TEXT]\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:54:59.757133Z","iopub.execute_input":"2025-07-13T20:54:59.757412Z","iopub.status.idle":"2025-07-13T20:55:08.198409Z","shell.execute_reply.started":"2025-07-13T20:54:59.757393Z","shell.execute_reply":"2025-07-13T20:55:08.197634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef preview_segmented_lines(folder):\n    images = sorted(os.listdir(folder))\n    print(f\"Total Segments: {len(images)}\")\n\n    for i, fname in enumerate(images):\n        img_path = os.path.join(folder, fname)\n        img = cv2.imread(img_path)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Line {i+1}\")\n        plt.axis('off')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:55:31.667172Z","iopub.execute_input":"2025-07-13T20:55:31.667728Z","iopub.status.idle":"2025-07-13T20:55:31.672313Z","shell.execute_reply.started":"2025-07-13T20:55:31.667703Z","shell.execute_reply":"2025-07-13T20:55:31.671575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preview_segmented_lines(\"line_images_page1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T20:24:28.519395Z","iopub.execute_input":"2025-07-13T20:24:28.519689Z","iopub.status.idle":"2025-07-13T20:24:29.462233Z","shell.execute_reply.started":"2025-07-13T20:24:28.519668Z","shell.execute_reply":"2025-07-13T20:24:29.461425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}